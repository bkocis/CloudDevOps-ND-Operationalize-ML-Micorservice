.CloudDevOps-ND-Operationalize-ML-Micorservice) ubuntu:~/environment/CloudDevOps-ND-Operationalize-ML-Micorservice (master) $ bash run_kubernetes.sh 
pod/mlapp4 created
NAME     READY   STATUS              RESTARTS   AGE
mlapp    0/1     ImagePullBackOff    0          3h20m
mlapp2   0/1     ImagePullBackOff    0          3h18m
mlapp3   1/1     Running             0          35m
mlapp4   0/1     ContainerCreating   0          0s
Error from server (NotFound): deployments.apps "mlapp4" not found
(.CloudDevOps-ND-Operationalize-ML-Micorservice) ubuntu:~/environment/CloudDevOps-ND-Operationalize-ML-Micorservice (master) $ kubectl get pods
NAME     READY   STATUS              RESTARTS   AGE
mlapp    0/1     ImagePullBackOff    0          3h20m
mlapp2   0/1     ImagePullBackOff    0          3h18m
mlapp3   1/1     Running             0          35m
mlapp4   0/1     ContainerCreating   0          17s
(.CloudDevOps-ND-Operationalize-ML-Micorservice) ubuntu:~/environment/CloudDevOps-ND-Operationalize-ML-Micorservice (master) $ kubectl get pods; kubectl port-forward mlapp4 8000:5001                                                                                 
NAME     READY   STATUS             RESTARTS   AGE
mlapp    0/1     ImagePullBackOff   0          3h20m
mlapp2   0/1     ImagePullBackOff   0          3h19m
mlapp3   1/1     Running            0          35m
mlapp4   1/1     Running            0          55s
Forwarding from 127.0.0.1:8000 -> 5001
Forwarding from [::1]:8000 -> 5001
Handling connection for 8000






# running the make_prediciton.sh from another terminal 

(.CloudDevOps-ND-Operationalize-ML-Micorservice) ubuntu:~/environment/CloudDevOps-ND-Operationalize-ML-Micorservice (master) $ bash make_prediction.sh 
Port: 8000
{
  "prediction": [
    20.35373177134412
  ]
}
(.CloudDevOps-ND-Operationalize-ML-Micorservice) ubuntu:~/environment/CloudDevOps-ND-Operationalize-ML-Micorservice (master) $ kubectl logs mlapp4
 * Serving Flask app "mlapp" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:5001/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 178-994-310
[2020-08-27 11:27:28,995] INFO in mlapp: JSON payload: 
{'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}
[2020-08-27 11:27:29,006] INFO in mlapp: Inference payload DataFrame: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2020-08-27 11:27:29,015] INFO in mlapp: Scaling Payload: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
127.0.0.1 - - [27/Aug/2020 11:27:29] "POST /predict HTTP/1.1" 200 -